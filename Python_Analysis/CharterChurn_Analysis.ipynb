{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform logistic regression and analysis with XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import math \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Analysis = pd.read_pickle(\"Data/AnalysisSet/Charter_Schools_Analysis.pkl\")    \n",
    "\n",
    "Charter_Schools_Analysis2 = Charter_Schools_Analysis[['School_x', 'ANN_TOTAL_ENROLL', 'Pct_White', 'Pct_Male','Pct_ELL', 'Pct_FRPM','year' ,'open_two_years', 'openyear', 'percent_unemployed', 'percent_nohs', 'SOCType', 'County', \n",
    "                                                     'Virtual', 'YearRoundYN','Magnet', 'PS_score']]\n",
    "\n",
    "Charter_Schools_Analysis2['First_School'] = (Charter_Schools_Analysis2.groupby('School_x').cumcount() == 0).astype(int)\n",
    "\n",
    "Charter_Schools_Analysis2 = Charter_Schools_Analysis2.loc[Charter_Schools_Analysis2['First_School'] > 0]\n",
    "Charter_Schools_Analysis2 = Charter_Schools_Analysis2.loc[Charter_Schools_Analysis2['openyear'] > 2003]\n",
    "\n",
    "Charter_Schools_Analysis2['virtual_school'] = np.where(Charter_Schools_Analysis2['Virtual'] == \"P\", 0, 1) \n",
    "Charter_Schools_Analysis2['Year_Round_school'] = np.where(Charter_Schools_Analysis2['YearRoundYN'] == \"N\", 0, 1)\n",
    "Charter_Schools_Analysis2['Magnet_school'] = np.where(Charter_Schools_Analysis2['Magnet'] == \"N\", 0, 1)\n",
    "\n",
    "Charter_Schools_Analysis2 = Charter_Schools_Analysis2.dropna()\n",
    "Charter_Schools_Analysis2 = Charter_Schools_Analysis2.reset_index(drop=True)\n",
    "\n",
    "Charter_Schools_Analysis3 = Charter_Schools_Analysis2.drop(columns=['Virtual', 'YearRoundYN', 'Magnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Pre_2017 = Charter_Schools_Analysis3.loc[Charter_Schools_Analysis3['year'] < 17]\n",
    "\n",
    "X_Pre_2017 = Charter_Schools_Pre_2017.drop(columns=['School_x', 'open_two_years', 'First_School', 'SOCType', 'County'])                                                                   \n",
    "y_Pre_2017 = Charter_Schools_Pre_2017['open_two_years']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Pre_2017, y_Pre_2017, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.Logit(y_train, X_train).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_logit = results.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_logit = pd.DataFrame(predicted_test_logit)\n",
    "predicted_test_logit = predicted_test_logit.rename(columns={0: \"Score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted_test_list_logit = predicted_test_logit.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_np = np.array(predicted_test_list_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_np, predicted_test_np)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run analysis using XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics  \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = np.ones(y_test.shape) * mean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions_classified = np.ones(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_baseline = log_loss(y_test, baseline_predictions_classified)\n",
    "print(\"Baseline accuracy is {:.2f}\".format(accuracy_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyper-parameters using a sequential grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'binary:logistic',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eval_metric'] = \"logloss\"\n",
    "\n",
    "num_boost_round = 999\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'logloss'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['test-logloss-mean'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try wider intervals with a larger step between\n",
    "# each value and then narrow it down. Here after several\n",
    "# iteration I found that the optimal value was in the\n",
    "# following ranges.\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(1,12)\n",
    "    for min_child_weight in range(1,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial best params and MAE\n",
    "min_error = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best error\n",
    "    mean_error = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tError {} for {} rounds\".format(mean_error, boost_rounds))\n",
    "    if mean_error < min_error:\n",
    "        min_error = mean_error\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, Error: {}\".format(best_params[0], best_params[1], min_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = 3\n",
    "params['min_child_weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(1,11)]\n",
    "    for colsample in [i/10. for i in range(1,11)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = float(\"Inf\")\n",
    "best_params = None\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    # Update best score\n",
    "    mean_error = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tError {} for {} rounds\".format(mean_error, boost_rounds))\n",
    "    if mean_error < min_error:\n",
    "        min_error = mean_error\n",
    "        best_params = (subsample,colsample)\n",
    "print(\"Best params: {}, {}, Error: {}\".format(best_params[0], best_params[1], min_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = 1.0\n",
    "params['colsample_bytree'] = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# This can take some timeâ€¦\n",
    "min_error = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [.9, .8, .7, .6, .5, .4, .3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    # Run and time CV\n",
    "    %time cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics=['logloss'],early_stopping_rounds=10)\n",
    "    # Update best score\n",
    "    mean_error = cv_results['test-logloss-mean'].min()\n",
    "    boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_error, boost_rounds))\n",
    "    if mean_error < min_error:\n",
    "        min_error = mean_error\n",
    "        best_params = eta\n",
    "print(\"Best params: {}, Error: {}\".format(best_params, min_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eta'] = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = model.best_iteration + 1\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(dtest, \"Test\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_model(\"my_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array(best_model.predict(dtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_np, x_np)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_xgb = np.where(x_np > .8, 1, 0) \n",
    "\n",
    "confusion_matrix(y_test_np, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logit = np.where(predicted_test_logit > .8, 1,0)\n",
    "\n",
    "confusion_matrix(y_test_np, y_pred_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Post_2016 = Charter_Schools_Analysis3.loc[Charter_Schools_Analysis3['year'] > 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Post_2016 = Charter_Schools_Post_2016.drop(columns=['School_x', 'open_two_years', 'First_School', 'SOCType', 'County'])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Post_2016['const'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Post_2016 = X_Post_2016[['const', 'ANN_TOTAL_ENROLL', 'Pct_White', 'Pct_Male', 'Pct_ELL', 'Pct_FRPM', 'year',\n",
    "                           'openyear', 'percent_unemployed', 'percent_nohs', 'PS_score', 'virtual_school',\n",
    "                           'Year_Round_school', 'Magnet_school']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_predict = xgb.DMatrix(X_Post_2016)\n",
    "Predicted_Post_2016 = best_model.predict(d_predict)\n",
    "Charter_Schools_Post_2016['predicted_survival'] = Predicted_Post_2016\n",
    "Charter_Schools_Post_2016['Safe'] = np.where(Charter_Schools_Post_2016['predicted_survival'] > .8, \"Safe\", \"Risky\")\n",
    "Charter_Schools_Post_2016_Out = Charter_Schools_Post_2016[['School_x', 'predicted_survival', 'Safe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Post_2016_Out2 = Charter_Schools_Analysis[['School_x', 'ANN_TOTAL_ENROLL', 'year', 'Latitude_x', 'Longitude_x', 'CDS_CODE']]\n",
    "Charter_Schools_Post_2016_Out2['First_School'] = (Charter_Schools_Post_2016_Out2.groupby('School_x').cumcount() == 0).astype(int)                                                                \n",
    "Charter_Schools_Post_2016_Out2 = Charter_Schools_Post_2016_Out2.loc[Charter_Schools_Post_2016_Out2['First_School'] > 0]\n",
    "Charter_Schools_Post_2016_Out2 = Charter_Schools_Post_2016_Out2.loc[Charter_Schools_Post_2016_Out2['year'] > 16]\n",
    "Charter_Schools_Analysis_Out = Charter_Schools_Post_2016_Out.merge(Charter_Schools_Post_2016_Out2, left_on='School_x', right_on='School_x')\n",
    "Charter_Schools_Analysis_Out = Charter_Schools_Analysis_Out.rename(columns={ \"School_x\" : \"School\", \"Latitude_x\" : \"Latitude\", \"Longitude_x\": \"Longitude\"})\n",
    "Charter_Schools_Analysis_Out = Charter_Schools_Analysis_Out[['School', 'ANN_TOTAL_ENROLL', 'CDS_CODE', 'predicted_survival', 'Safe', 'Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Analysis_Out.to_pickle(\"Data/AnalysisSet/Charter_Schools_Analysis_Out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from geopandas import GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Analysis = pd.read_pickle(\"Data/AnalysisSet/Charter_Schools_Analysis_Out.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Analysis['Latitude'] = Charter_Schools_Analysis['Latitude'].astype(float)\n",
    "Charter_Schools_Analysis['Longitude'] = Charter_Schools_Analysis['Longitude'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Analysis['First_Lat'] = (Charter_Schools_Analysis.groupby('Latitude').cumcount() == 0).astype(int)\n",
    "Charter_Schools_Analysis = Charter_Schools_Analysis.loc[Charter_Schools_Analysis['First_Lat'] > 0]\n",
    "Charter_Schools_Analysis = Charter_Schools_Analysis.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "Charter_Schools_Analysis['location_home'] = list(zip(Charter_Schools_Analysis['Latitude'], Charter_Schools_Analysis['Longitude']))\n",
    "\n",
    "Charter_Analysis_Neighbor = Charter_Schools_Analysis[['School', 'location_home', 'Latitude', 'Longitude', 'predicted_survival', 'Safe', 'ANN_TOTAL_ENROLL']]\n",
    "\n",
    "for i in range(len(Charter_Analysis_Neighbor)):\n",
    "    distance_array = []\n",
    "    d_i = Charter_Analysis_Neighbor['location_home'][i]\n",
    "    for j in range(len(Charter_Analysis_Neighbor)):\n",
    "        d = (geodesic(d_i, Charter_Analysis_Neighbor['location_home'][j]).miles)\n",
    "        distance_array.append(d)\n",
    "        \n",
    "    distance_df = pd.DataFrame(distance_array)\n",
    "    distance_df = distance_df.rename(columns={0: \"Distance\"})\n",
    "        \n",
    "    Charter_Analysis_Neighbor2 =  Charter_Analysis_Neighbor.join(distance_df, how='outer')\n",
    "    Charter_Analysis_Neighbor3 = Charter_Analysis_Neighbor2.nsmallest(6, 'Distance')\n",
    "    Charter_Analysis_Neighbor4 = Charter_Analysis_Neighbor3[0:6] \n",
    "    Charter_Analysis_Neighbor4 = Charter_Analysis_Neighbor4.reset_index(drop=True)\n",
    "        \n",
    "    Charter_Analysis_Neighbor4['Indexi'] = Charter_Analysis_Neighbor4.index\n",
    "    Charter_Analysis_Neighbor4['Indexj'] = i\n",
    "    Charter_Analysis_Neighbor5 = Charter_Analysis_Neighbor4.pivot(index= \"Indexj\", columns='Indexi')\n",
    "   \n",
    "    if i == 0:\n",
    "        Charter_Schools_Wide = Charter_Analysis_Neighbor5\n",
    "    else:\n",
    "        Charter_Schools_Wide = Charter_Schools_Wide.append(Charter_Analysis_Neighbor5, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charter_Schools_Wide.to_csv('Data/AnalysisSet/Charter_Analysis_Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
